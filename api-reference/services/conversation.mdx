---
title: "Conversation"
description: "Multi-turn conversation endpoint. Fast and cost-effective for conversational tasks without tool calling."
---

<a href="https://platform.incredible.one/settings/api-keys" target="_blank" style={{ textDecoration: 'none' }}>
  <div style={{ 
    border: '1px solid #e5e7eb', 
    borderRadius: '12px', 
    padding: '20px', 
    margin: '24px 0', 
    display: 'flex', 
    alignItems: 'center', 
    gap: '16px',
    cursor: 'pointer',
    transition: 'all 0.2s'
  }}>
    <div style={{ fontSize: '32px' }}>ðŸ”‘</div>
    <div style={{ flex: 1 }}>
      <div style={{ fontSize: '18px', fontWeight: '600', marginBottom: '4px' }}>Get your Incredible API key</div>
      <div style={{ fontSize: '14px', color: '#6b7280' }}>Generate your API key to start using this endpoint</div>
    </div>
    <div style={{ fontSize: '20px', color: '#9ca3af' }}>â†’</div>
  </div>
</a>

## Overview

The Conversation API is designed for multi-turn dialogues where context and conversation history matter. Unlike the Answer API which handles single questions, Conversation maintains the flow of discussion across multiple exchanges, making it ideal for chatbots, virtual assistants, and interactive applications.

**Key characteristics:**
- **Stateless but context-aware** - You provide the full conversation history with each request
- **Optimized for dialogue** - Faster and more cost-effective than the Agent endpoint when you don't need tool calling
- **Flexible context** - Supports alternating user and assistant messages
- **Document-aware** - Can reference uploaded files for context-rich conversations

**When to use Conversation vs other endpoints:**
- Use **Conversation** for multi-turn dialogue without needing tools or function calling
- Use **Answer** for single-question scenarios or stateless Q&A
- Use **Agent** when you need autonomous tool calling and complex workflows

## How it works

The Conversation endpoint processes a sequence of messages representing the dialogue history. Each message has a `role` (either "user" or "assistant") and `content`. The API uses this context to generate a contextually appropriate response.

You're responsible for managing the conversation state in your application. Each request should include the full history you want the model to consider. This gives you complete control over context management and allows you to implement features like context pruning or conversation forking.

## Examples

<CodeGroup>

```python Python SDK
from incredible_python import Incredible

client = Incredible(api_key="YOUR_API_KEY")

response = client.conversation(
    messages=[
        {"role": "user", "content": "Hello!"},
        {"role": "assistant", "content": "Hi there! How can I help you today?"},
        {"role": "user", "content": "Tell me a joke"}
    ]
)

print(response.response)
```

```ts TypeScript SDK
import { IncredibleClient } from '@incredible-ai/sdk';

const client = new IncredibleClient();

const response = await client.conversation({
  messages: [
    { role: 'user', content: 'Hello!' },
    { role: 'assistant', content: 'Hi there!' },
    { role: 'user', content: 'How are you?' }
  ]
});

console.log(response.response || response.message);
```

```python OpenAI SDK
from openai import OpenAI

client = OpenAI(
    base_url="https://api.incredible.one/v1",
    api_key="YOUR_API_KEY"
)

response = client.chat.completions.create(
    model="conversation",
    messages=[
        {"role": "user", "content": "Hello!"},
        {"role": "assistant", "content": "Hi there!"},
        {"role": "user", "content": "How are you?"}
    ]
)
print(response.choices[0].message.content)
```

```bash cURL
curl -X POST "https://api.incredible.one/v1/conversation" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"},
      {"role": "assistant", "content": "Hi there!"},
      {"role": "user", "content": "How are you?"}
    ]
  }'
```

</CodeGroup>

## Request Parameters

### messages (required)
An array of message objects representing the conversation history. Each message must have:
- **role**: Either `"user"` (for user messages) or `"assistant"` (for AI responses)
- **content**: The text content of the message
- **file_ids** (optional): Array of file IDs to attach to a specific message

Messages are processed in order, allowing the model to understand the full context of the conversation. You can include as much history as needed, though very long conversations may need to be pruned to stay within context limits.

**Best practices:**
- Always provide enough context for the model to understand the current query
- Consider implementing conversation summarization for very long dialogues
- Alternate between user and assistant messages naturally
- Don't include messages that are no longer relevant to the current topic

### system_prompt (optional)
A system-level instruction that defines the AI's behavior, personality, and constraints throughout the conversation. This is set once and influences all responses.

**Examples of system prompts:**
- `"You are a helpful customer service representative for Acme Corp."`
- `"You are a technical expert who explains concepts in simple terms."`
- `"You are a creative writing assistant who helps brainstorm story ideas."`

The system prompt shapes the assistant's tone, expertise level, and boundaries. It's applied before any user messages and remains active throughout the conversation.

### stream (optional)
Enable real-time streaming of responses. When `true`, you'll receive message chunks as they're generated rather than waiting for the complete response. Default is `false`.

## Attaching Files to Conversations

Files can be attached to individual messages within a conversation, allowing the AI to reference document content when generating responses. This is powerful for document Q&A, report analysis, and context-specific assistance.

You can provide document context in conversations by uploading files and referencing them:

<CodeGroup>

```python Python SDK
from incredible_python import Incredible

client = Incredible(api_key="YOUR_API_KEY")

# Upload a file
with open("report.pdf", "rb") as f:
    file = client.files.upload(file=f, purpose="assistants")

# Use the file in a conversation
response = client.conversation(
    messages=[
        {"role": "user", "content": "What's in this document?", "file_ids": [file.id]},
        {"role": "assistant", "content": "This document contains a quarterly financial report."},
        {"role": "user", "content": "What were the key findings?"}
    ]
)

print(response.response)
```

```ts TypeScript SDK
import { IncredibleClient } from '@incredible-ai/sdk';
import fs from 'fs';

const client = new IncredibleClient({ apiKey: "YOUR_API_KEY" });

// Upload a file
const fileStream = fs.createReadStream('report.pdf');
const file = await client.files.upload({
  file: fileStream,
  purpose: 'assistants'
});

// Use the file in a conversation
const response = await client.conversation({
  messages: [
    { role: 'user', content: "What's in this document?", file_ids: [file.id] },
    { role: 'assistant', content: 'This document contains a quarterly financial report.' },
    { role: 'user', content: 'What were the key findings?' }
  ]
});

console.log(response.response);
```

</CodeGroup>

## Streaming Responses

Streaming creates a more natural, chat-like experience by delivering responses in real-time as they're generated. Instead of waiting for the complete message, your users see text appearing progressively, similar to how a human types.

**Benefits of streaming in conversations:**
- **Reduced perceived latency** - Users see progress immediately
- **Better engagement** - The typing effect feels more natural and interactive  
- **Improved UX** - Users can start reading while the response is still being generated
- **Cancellation support** - Users can interrupt long responses they don't need

Enable streaming by setting `stream: true` in your request:

<CodeGroup>

```python Python SDK
from incredible_python import Incredible

client = Incredible(api_key="YOUR_API_KEY")

# Stream a conversation
stream = client.conversation(
    messages=[
        {"role": "user", "content": "Tell me a story about a robot"},
        {"role": "assistant", "content": "Once upon a time..."},
        {"role": "user", "content": "What happened next?"}
    ],
    stream=True
)

# Process streaming chunks
for chunk in stream:
    if hasattr(chunk, 'content') and chunk.content:
        print(chunk.content, end='', flush=True)
    if hasattr(chunk, 'done') and chunk.done:
        print("\n[Stream complete]")
```

```ts TypeScript SDK
import { IncredibleClient } from '@incredible-ai/sdk';

const client = new IncredibleClient({ apiKey: "YOUR_API_KEY" });

// Stream a conversation
const stream = await client.conversation({
  messages: [
    { role: 'user', content: 'Tell me a story about a robot' },
    { role: 'assistant', content: 'Once upon a time...' },
    { role: 'user', content: 'What happened next?' }
  ],
  stream: true
});

// Process streaming events
for await (const event of stream) {
  if (event.content) {
    process.stdout.write(event.content);
  }
  if (event.done) {
    console.log('\n[Stream complete]');
  }
}
```

```bash cURL
curl -X POST "https://api.incredible.one/v1/conversation" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Tell me a story about a robot"},
      {"role": "assistant", "content": "Once upon a time..."},
      {"role": "user", "content": "What happened next?"}
    ],
    "stream": true
  }'
```

</CodeGroup>

**Stream Event Types:**
- `content` - Text chunks as they're generated
- `thinking` - Internal reasoning process (if available)
- `done` - Signals completion of the stream
- `error` - Any errors that occurred during generation

## Response

```json
{
  "success": true,
  "response": "I'm doing well, thank you for asking! How can I help you today?"
}
```
